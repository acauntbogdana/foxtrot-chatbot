import gradio as gr
import pandas as pd
import faiss
import numpy as np
import json
import os
import re
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import defaultdict
import math
from datetime import datetime
from sentence_transformers import SentenceTransformer

# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
EXCEL_FILE = "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π.xlsx"
LOGO_PATH = "logo_fox.png"
EMBEDDING_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
HISTORY_FILE = "dialog_history.jsonl"

# === –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –° –ö–û–ù–¢–†–û–õ–ï–ú –î–õ–ò–ù–´ ===
class ResonanceAttention(nn.Module):
    def __init__(self, dim, num_waves=8):
        super().__init__()
        self.num_waves = num_waves
        self.dim = dim
        self.wave_frequencies = nn.Parameter(torch.randn(num_waves) * 0.02)
        self.wave_amplitudes = nn.Parameter(torch.ones(num_waves))
        self.wave_projection = nn.Linear(num_waves, dim)
        
    def forward(self, x):
        batch_size, seq_len, dim = x.shape
        positions = torch.arange(seq_len, device=x.device).float()
        
        wave_patterns = []
        for i in range(self.num_waves):
            freq = self.wave_frequencies[i]
            amplitude = self.wave_amplitudes[i]
            wave = amplitude * torch.sin(2 * math.pi * freq * positions / seq_len)
            wave_patterns.append(wave)
        
        wave_matrix = torch.stack(wave_patterns, dim=1)
        wave_features = self.wave_projection(wave_matrix)
        resonance = wave_features.unsqueeze(0).expand(batch_size, -1, -1)
        return x + resonance

class QuantumLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.linear_real = nn.Linear(input_dim, output_dim)
        self.linear_imag = nn.Linear(input_dim, output_dim)
        self.phase_shift = nn.Parameter(torch.randn(output_dim) * 0.1)
        
    def forward(self, x):
        real_part = self.linear_real(x)
        imag_part = self.linear_imag(x)
        amplitude = torch.sqrt(real_part**2 + imag_part**2 + 1e-8)
        phase = torch.atan2(imag_part, real_part) + self.phase_shift
        output = amplitude * torch.cos(phase)
        return F.gelu(output)

class WaveTransformer(nn.Module):
    def __init__(self, vocab_size, hidden_dim=256, num_layers=3):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.vocab_size = vocab_size
        
        self.embedding = nn.Embedding(vocab_size, hidden_dim)
        self.pos_encoding = nn.Parameter(torch.randn(1, 150, hidden_dim) * 0.02)
        
        self.quantum_layers = nn.ModuleList([
            QuantumLayer(hidden_dim, hidden_dim) for _ in range(num_layers)
        ])
        
        self.resonance_attention = nn.ModuleList([
            ResonanceAttention(hidden_dim, num_waves=6) for _ in range(num_layers)
        ])
        
        self.output_projection = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, vocab_size)
        )
        
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        batch_size, seq_len = x.shape
        x = self.embedding(x)
        
        if seq_len <= 150:
            x = x + self.pos_encoding[:, :seq_len, :]
        
        x = self.dropout(x)
        
        for quantum_layer, resonance_attn in zip(self.quantum_layers, self.resonance_attention):
            x = quantum_layer(x)
            x = resonance_attn(x)
            x = self.layer_norm(x)
        
        logits = self.output_projection(x)
        return logits

# === –£–õ–£–ß–®–ï–ù–ù–´–ô –¢–û–ö–ï–ù–ò–ó–ê–¢–û–† ===
class RussianTokenizer:
    def __init__(self):
        self.vocab = defaultdict(lambda: len(self.vocab))
        self.reverse_vocab = {}
        
        self.PAD = self.vocab['<PAD>']
        self.UNK = self.vocab['<UNK>']
        self.SOS = self.vocab['<SOS>']
        self.EOS = self.vocab['<EOS>']
        
        self._init_base_vocab()
    
    def _init_base_vocab(self):
        base_chars = "–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è "
        base_chars += base_chars.upper()
        base_chars += "0123456789.,!?;:-()\"'"
        
        for char in base_chars:
            self.vocab[char]
        
        self.reverse_vocab = {v: k for k, v in self.vocab.items()}
    
    def encode(self, text, max_length=120):
        tokens = [self.SOS]
        for char in text[:max_length-2]:
            tokens.append(self.vocab.get(char, self.UNK))
        tokens.append(self.EOS)
        
        while len(tokens) < max_length:
            tokens.append(self.PAD)
            
        return tokens[:max_length]
    
    def decode(self, tokens):
        text = []
        for t in tokens:
            if t == self.EOS:
                break
            if t not in [self.PAD, self.SOS]:
                text.append(self.reverse_vocab.get(t, '?'))
        return ''.join(text)

# === –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ì–ï–ù–ï–†–ê–¶–ò–ò ===
class WaveChatSystem:
    def __init__(self):
        self.tokenizer = RussianTokenizer()
        self.vocab_size = len(self.tokenizer.vocab)
        
        self.model = WaveTransformer(
            vocab_size=self.vocab_size,
            hidden_dim=256,
            num_layers=3
        )
        
        self.optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss(ignore_index=self.tokenizer.PAD)
        
        self.index = None
        self.corpus = None
        self.embedding_model = None
        self.knowledge_df = None
        
        print(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {self.vocab_size}")

    def load_knowledge_base(self):
        print("üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π...")
        df = pd.read_excel(EXCEL_FILE)
        df = df.dropna(subset=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç'])
        df['–í–æ–ø—Ä–æ—Å'] = df['–í–æ–ø—Ä–æ—Å'].astype(str).str.strip()
        df['–û—Ç–≤–µ—Ç'] = df['–û—Ç–≤–µ—Ç'].astype(str).str.strip()
        df = df[df['–û—Ç–≤–µ—Ç'].str.len() > 10]
        
        self.knowledge_df = df
        self.corpus = df['–í–æ–ø—Ä–æ—Å'].tolist()
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")

    def create_search_index(self):
        if self.index is not None:
            return
            
        self.load_knowledge_base()
        self.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
        
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
        embeddings = self.embedding_model.encode(self.corpus, show_progress_bar=True, convert_to_numpy=True)
        
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
        print("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω")

    def search_knowledge(self, query, top_k=3):
        if self.index is None:
            self.create_search_index()
            
        query_emb = self.embedding_model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(query_emb)
        
        k = min(top_k, len(self.corpus))
        distances, indices = self.index.search(query_emb, k)
        
        results = []
        for i, distance in zip(indices[0], distances[0]):
            if distance > 0.2:  # –ü–æ–Ω–∏–∂–µ–Ω –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞
                question = self.corpus[i]
                match = self.knowledge_df[self.knowledge_df['–í–æ–ø—Ä–æ—Å'] == question]
                if not match.empty:
                    results.append(match.iloc[0]['–û—Ç–≤–µ—Ç'])
        
        return results

    def train_on_qa_pairs(self):
        if self.knowledge_df is None:
            self.load_knowledge_base()
            
        self.model.train()
        total_loss = 0
        
        # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏
        sample_df = self.knowledge_df[self.knowledge_df['–û—Ç–≤–µ—Ç'].str.len() > 50].sample(
            min(20, len(self.knowledge_df))
        )
        
        for _, row in sample_df.iterrows():
            question = row['–í–æ–ø—Ä–æ—Å'][:60]
            answer = row['–û—Ç–≤–µ—Ç'][:100]  # –ë–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            
            input_tokens = self.tokenizer.encode("–í–æ–ø—Ä–æ—Å: " + question)
            target_tokens = self.tokenizer.encode("–û—Ç–≤–µ—Ç: " + answer)
            
            input_tensor = torch.tensor([input_tokens], dtype=torch.long)
            target_tensor = torch.tensor([target_tokens], dtype=torch.long)
            
            self.optimizer.zero_grad()
            output = self.model(input_tensor)
            
            loss = self.criterion(output.view(-1, self.vocab_size), target_tensor.view(-1))
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / len(sample_df)

    def generate_complete_answer(self, question, context_answers, max_length=120, temperature=0.7):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–ª–∏–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞"""
        self.model.eval()
        
        if not context_answers:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        combined_context = " ".join(context_answers[:2])[:300]
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –Ω–∞ –ø–æ–ª–Ω–æ—Ç—É –æ—Ç–≤–µ—Ç–∞
        prompt = f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {combined_context} | –í–æ–ø—Ä–æ—Å: {question} | –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç:"
        
        input_tokens = self.tokenizer.encode(prompt)
        generated_tokens = []
        
        with torch.no_grad():
            for step in range(max_length):
                input_tensor = torch.tensor([input_tokens], dtype=torch.long)
                output = self.model(input_tensor)
                
                next_token_logits = output[0, -1] / temperature
                
                # Penalize repeating tokens
                for token in set(generated_tokens[-10:]):  # –°–º–æ—Ç—Ä–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Ç–æ–∫–µ–Ω–æ–≤
                    next_token_logits[token] -= 0.5
                
                next_token_probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(next_token_probs, 1).item()
                
                # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                if next_token == self.tokenizer.EOS:
                    if len(generated_tokens) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
                        break
                    else:
                        continue  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
                
                generated_tokens.append(next_token)
                input_tokens.append(next_token)
                
                # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                if len(input_tokens) > 100:
                    input_tokens = input_tokens[-100:]
                
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è—Ö
                if len(generated_tokens) > 40:
                    last_chars = self.tokenizer.decode(generated_tokens[-5:])
                    if any(mark in last_chars for mark in ['.', '!', '?', ';']):
                        # –° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 30% –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–æ—Å–ª–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
                        if torch.rand(1).item() < 0.3:
                            break
        
        response = self.tokenizer.decode(generated_tokens)
        
        # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
        if not response.strip():
            return context_answers[0][:200]  # Fallback –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ –æ–±—Ä—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –ø–æ–ª—É—Å–ª–æ–≤–µ
        if len(response) > 10 and not response.endswith(('.', '!', '?', ';')):
            response += '.'
        
        return response

    def chat(self, message):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —á–∞—Ç–∞"""
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        context_answers = self.search_knowledge(message)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        question_type = self.classify_question(message)
        
        if context_answers:
            response = self.generate_complete_answer(message, context_answers)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            if not self.is_response_relevant(response, message):
                response = context_answers[0][:250] + "..."
                
        else:
            response = "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –º–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –Ω–µ—Ç –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É. –†–µ–∫–æ–º–µ–Ω–¥—É—é –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ –Ω–∞–ª–æ–≥–æ–≤—É—é —Å–ª—É–∂–±—É –∏–ª–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–æ–Ω–Ω—ã–π —Ü–µ–Ω—Ç—Ä –ü—Ä–∏–º–æ—Ä—Å–∫–æ–≥–æ –∫—Ä–∞—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."
        
        return response

    def classify_question(self, question):
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ', '–æ–∑–Ω–∞—á–∞–µ—Ç']):
            return 'definition'
        elif any(word in question_lower for word in ['–∫–∞–∫', '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—à–∞–≥–∏']):
            return 'howto'
        elif any(word in question_lower for word in ['–∫–∞–∫–æ–π', '–∫–∞–∫–∏–µ', '–ø–µ—Ä–µ—á–∏—Å–ª–∏']):
            return 'list'
        else:
            return 'general'

    def is_response_relevant(self, response, question):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –≤–æ–ø—Ä–æ—Å—É"""
        question_words = set(re.findall(r'\b\w{3,}\b', question.lower()))
        response_words = set(re.findall(r'\b\w{3,}\b', response.lower()))
        
        common_words = question_words.intersection(response_words)
        return len(common_words) >= 2

# === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã...")
chat_system = WaveChatSystem()
chat_system.create_search_index()

# –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö
print("üéØ –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–∞—Ö...")
try:
    for epoch in range(2):
        loss = chat_system.train_on_qa_pairs()
        print(f"–≠–ø–æ—Ö–∞ {epoch + 1}, –ü–æ—Ç–µ—Ä–∏: {loss:.4f}")
except Exception as e:
    print(f"‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ: {e}")

print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")

# === –ò–ù–¢–ï–†–§–ï–ô–° ===
def save_dialog(user_msg, bot_msg):
    entry = {
        "timestamp": datetime.now().isoformat(),
        "user": user_msg,
        "bot": bot_msg
    }
    
    try:
        with open(HISTORY_FILE, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")
    except Exception:
        pass

def chat_interface(message, history):
    if not message.strip():
        return "", history
    
    try:
        response = chat_system.chat(message)
        save_dialog(message, response)
        
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": response})
        
        return "", history
        
    except Exception as e:
        error_msg = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": error_msg})
        return "", history

with gr.Blocks(title="ü¶ä –§–æ–∫—Å—Ç—Ä–æ—Ç", theme=gr.themes.Soft()) as demo:
    
    gr.Markdown("""
    # ü¶ä –§–æ–∫—Å—Ç—Ä–æ—Ç - –ë–∏–∑–Ω–µ—Å-—Å–æ–≤–µ—Ç–Ω–∏–∫ –ü—Ä–∏–º–æ—Ä—Å–∫–æ–≥–æ –∫—Ä–∞—è
    *–ü–æ–ª–Ω—ã–µ –∏ —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã*
    """)
    
    chatbot = gr.Chatbot(
        label="–ß–∞—Ç —Å –§–æ–∫—Å—Ç—Ä–æ—Ç–æ–º",
        avatar_images=(None, LOGO_PATH),
        height=500,
        type="messages"
    )
    
    msg = gr.Textbox(
        placeholder="–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å –æ –±–∏–∑–Ω–µ—Å–µ, –Ω–∞–ª–æ–≥–∞—Ö, –ò–ü –∏–ª–∏ –û–û–û...",
        label="–í–∞—à –≤–æ–ø—Ä–æ—Å",
        max_lines=2
    )
    
    with gr.Row():
        submit_btn = gr.Button("üì® –û—Ç–ø—Ä–∞–≤–∏—Ç—å", variant="primary")
        clear_btn = gr.Button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é")
    
    msg.submit(chat_interface, [msg, chatbot], [msg, chatbot])
    submit_btn.click(chat_interface, [msg, chatbot], [msg, chatbot])
    clear_btn.click(lambda: ([], ""), outputs=[chatbot, msg])

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
